{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03046750-0d49-4615-b4b2-b54ffa8409a4",
   "metadata": {},
   "source": [
    "# Hyperdimensional Computing (HDC) on MNIST: Key Intuition\n",
    "\n",
    "## Introduction to MNIST\n",
    "- **MNIST** is a classic dataset in machine learning, containing **28x28 grayscale images of handwritten digits (0–9)**.\n",
    "- Each image is labeled with the correct digit.\n",
    "- It is commonly used for **testing and learning image classification algorithms**.\n",
    "\n",
    "## What is Hyperdimensional Computing (HDC)?\n",
    "- HDC is an alternative to traditional neural networks for classification.\n",
    "- Instead of learning weights and biases, HDC **encodes data into very high-dimensional vectors** (tens of thousands of dimensions).\n",
    "- Each class (digit) is represented by a **centroid vector**, which is essentially the “average” vector of all training images of that class.\n",
    "\n",
    "## How HDC Works Step-by-Step\n",
    "1. **Encoding Images**\n",
    "   - Each image is **flattened** into a 1D vector of pixels.\n",
    "   - Each pixel is assigned:\n",
    "     - A **position vector** (to remember where the pixel is in the image)\n",
    "     - A **value vector** (to represent the pixel intensity)\n",
    "   - These vectors are **bound and combined** to form a single high-dimensional vector representing the image.\n",
    "\n",
    "2. **Training (Building Centroids)**\n",
    "   - For each training image:\n",
    "     - Encode it into a high-dimensional vector.\n",
    "     - Add it to the **centroid of its class** (e.g., all '3's contribute to the '3' centroid).\n",
    "   - After all images are processed, each centroid **represents the “average” of all images in that class**.\n",
    "\n",
    "3. **Testing (Classification)**\n",
    "   - For a new image:\n",
    "     - Encode it into a high-dimensional vector.\n",
    "     - Compare it with all class centroids using **similarity** (dot product/cosine similarity).\n",
    "     - The **most similar centroid** determines the predicted digit.\n",
    "\n",
    "## Why HDC is Intuitive for Beginners\n",
    "- No complex optimization or backpropagation is needed.\n",
    "- Classification is based on **similarity in high-dimensional space**, which is conceptually easier than tuning thousands of neural network parameters.\n",
    "- HDC is **fast and memory-efficient**, making it ideal for lightweight learning tasks and edge devices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ccbb22-400c-4386-86a9-4aff473d49f6",
   "metadata": {},
   "source": [
    "1. Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6b275c2-8b22-4edd-90aa-9a9d13d48837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "# torchmetrics is used to calculate accuracy in a convenient way\n",
    "import torchmetrics\n",
    "from tqdm import tqdm  # for showing a progress bar during training/testing\n",
    "\n",
    "import torchhd  # library for Hyperdimensional Computing (HDC)\n",
    "from torchhd.models import Centroid  # HDC model that stores class centroids\n",
    "from torchhd import embeddings  # for creating high-dimensional vector representations\n",
    "\n",
    "\n",
    "# Choose GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using {} device\".format(device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecd79f6-7cd8-4767-9dfe-c3691de92e5b",
   "metadata": {},
   "source": [
    "2. HDC and dataset parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "793812f3-1a82-400b-8590-dd9af107aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSIONS = 10000  # number of dimensions for high-dimensional vectors (very large)\n",
    "IMG_SIZE = 28       # MNIST images are 28x28 pixels\n",
    "NUM_LEVELS = 1000   # number of quantization levels for pixel intensity\n",
    "BATCH_SIZE = 1      # process one image at a time (can increase if memory allows)\n",
    "\n",
    "# Transform MNIST images into PyTorch tensors\n",
    "transform = torchvision.transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df19e96e-6d83-4f7a-b424-f202016f87a4",
   "metadata": {},
   "source": [
    "3. Load MNIST dataset from the mnist_data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0739f7b-b85e-4c34-9e72-e3cfe0badc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MNIST(\"./mnist_data\", train=True, transform=transform, download=False)\n",
    "train_ld = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# train_ld is an iterator that returns batches of training images and labels\n",
    "\n",
    "test_ds = MNIST(\"./mnist_data\", train=False, transform=transform, download=False)\n",
    "test_ld = torch.utils.data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# test_ld is an iterator for testing images (we don't shuffle test data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5a5a4e-4f37-4e17-adb5-c40b7f1e8e88",
   "metadata": {},
   "source": [
    "4. Define HDC encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfd1d2d1-df3c-463a-89a5-12409dcdab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, out_features, size, levels):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Flatten image from 28x28 to 784 vector\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        \n",
    "        # Position embedding: assigns a unique high-dimensional vector to each pixel position\n",
    "        self.position = embeddings.Random(size * size, out_features)\n",
    "        \n",
    "        # Value embedding: assigns a vector for each possible pixel intensity\n",
    "        self.value = embeddings.Level(levels, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the image\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Bind position and value vectors together for each pixel\n",
    "        sample_hv = torchhd.bind(self.position.weight, self.value(x))\n",
    "        \n",
    "        # Combine all pixel vectors into one high-dimensional representation\n",
    "        sample_hv = torchhd.multiset(sample_hv)\n",
    "        \n",
    "        # Convert to binary vector (+1/-1) for HDC processing\n",
    "        return torchhd.hard_quantize(sample_hv)\n",
    "\n",
    "\n",
    "# Instantiate the encoder and move to device (CPU/GPU)\n",
    "encode = Encoder(DIMENSIONS, IMG_SIZE, NUM_LEVELS)\n",
    "encode = encode.to(device)\n",
    "\n",
    "# Number of classes (digits 0-9)\n",
    "num_classes = len(train_ds.classes)\n",
    "\n",
    "# HDC model using Centroid approach\n",
    "# It will store one high-dimensional vector per class representing all images of that class\n",
    "model = Centroid(DIMENSIONS, num_classes)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56cfc1f-0120-46c9-b99a-85e663c4950f",
   "metadata": {},
   "source": [
    "5. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b7cf677-ab24-42d8-b713-a628933c302c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                                                                                                     | 0/60000 [00:00<?, ?it/s]/tmp/ipykernel_2631181/3403991540.py:24: DeprecationWarning: torchhd.hard_quantize is deprecated, consider using torchhd.normalize instead.\n",
      "  return torchhd.hard_quantize(sample_hv)\n",
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60000/60000 [00:18<00:00, 3224.26it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():  # HDC does not require gradient calculation like neural networks\n",
    "    for samples, labels in tqdm(train_ld, desc=\"Training\"):\n",
    "        samples = samples.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Encode the image into high-dimensional vector\n",
    "        samples_hv = encode(samples)\n",
    "        \n",
    "        # Add this vector to the centroid corresponding to its class\n",
    "        model.add(samples_hv, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6116de15-96d9-4a75-87af-dace08f10f99",
   "metadata": {},
   "source": [
    "6. Evaluation setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f943a42-69bd-46cf-8af1-bdd0ef27af5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|                                                                                                                                                                      | 0/10000 [00:00<?, ?it/s]/tmp/ipykernel_2631181/3403991540.py:24: DeprecationWarning: torchhd.hard_quantize is deprecated, consider using torchhd.normalize instead.\n",
      "  return torchhd.hard_quantize(sample_hv)\n",
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:05<00:00, 1996.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy of 82.710%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = torchmetrics.Accuracy(\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Normalize the centroids after training\n",
    "    # This ensures all centroids have equal length for similarity comparison\n",
    "    model.normalize()\n",
    "\n",
    "    # Testing loop\n",
    "    for samples, labels in tqdm(test_ld, desc=\"Testing\"):\n",
    "        samples = samples.to(device)\n",
    "\n",
    "        # Encode test image\n",
    "        samples_hv = encode(samples)\n",
    "        \n",
    "        # Compute similarity between test vector and all class centroids\n",
    "        # dot=True computes cosine similarity\n",
    "        outputs = model(samples_hv, dot=True)\n",
    "        \n",
    "        # Update accuracy metric with predicted labels\n",
    "        accuracy.update(outputs.cpu(), labels)\n",
    "\n",
    "# Print final accuracy\n",
    "print(f\"Testing accuracy of {(accuracy.compute().item() * 100):.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2297f99-2ff6-4df3-afeb-4a788ee1d0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
