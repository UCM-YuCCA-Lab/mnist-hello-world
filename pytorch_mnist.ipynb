{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a **step-by-step PyTorch tutorial** that teaches how to train and evaluate a simple neural network on the MNIST dataset of handwritten digits. It is designed for **undergraduates with no prior experience in machine learning**.\n",
    "\n",
    "\n",
    "Here’s the big picture intuition:\n",
    "- **Neural networks learn patterns in data.** In this case, the network learns to recognize handwritten digits (0–9).\n",
    "- We break the process into clear steps:\n",
    "1. **Prepare the data** – Load images of digits and their labels (the correct answers).\n",
    "2. **Build the model** – Define a simple neural network with layers of mathematical functions.\n",
    "3. **Train the model** – Show the network many examples so it learns from its mistakes using a process called *gradient descent*.\n",
    "4. **Evaluate the model** – Test the trained model on new, unseen images to check how well it performs.\n",
    "- The **training loop** is where learning happens: the model makes predictions, we measure how wrong it is (loss), and then adjust the model’s parameters to improve.\n",
    "- The **evaluation phase** checks accuracy, ensuring the model generalizes to data it hasn’t seen before.\n",
    "\n",
    "\n",
    "By the end of this notebook, you’ll understand the essential workflow of machine learning: **data → model → training → evaluation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to import PyTorch libraries to handle tensors, models, and optimization,\n",
    "# and torchvision to load the MNIST dataset and perform transformations.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm # Import tqdm for progress bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available. If yes, we will use it for faster computation, otherwise CPU will be used.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Define transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST images are 28x28 pixels in grayscale. We convert them to PyTorch tensors so the model can process them.\n",
    "transform = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Load MNIST dataset\n",
    "\n",
    "* 'root' is the folder where the MNIST data is stored.\n",
    "* 'train=True' loads the training data; 'train=False' loads the test data.\n",
    "* 'download=False' because we already have the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 60000\n",
      "Number of test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "mnist_root = \"mnist_data\" # change if your folder is elsewhere\n",
    "\n",
    "train_dataset = datasets.MNIST(root=mnist_root, train=True, transform=transform, download=False)\n",
    "test_dataset = datasets.MNIST(root=mnist_root, train=False, transform=transform, download=False)\n",
    "\n",
    "# DataLoader wraps the dataset and allows us to iterate in batches\n",
    "# 'batch_size' specifies how many images are processed at once\n",
    "# 'shuffle=True' randomizes the order of images during training\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Define a simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A neural network is a function that maps input images to output labels (digits 0-9)\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Flatten layer converts 28x28 image into a single 784-dimensional vector\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Fully connected layer from 784 inputs to 128 neurons\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        # ReLU is an activation function introducing non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "        # Output layer with 10 neurons for 10 digit classes\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Define how data flows through the network\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNN().to(device) # Move the model to GPU if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Define loss and optimizer\n",
    "\n",
    "* Loss measures how far the model's predictions are from the correct answers\n",
    "* CrossEntropyLoss is suitable for multi-class classification\n",
    "* Adam optimizer adjusts the model's weights based on the gradients to reduce loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [00:02<00:00, 462.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Average Loss: 0.3477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [00:02<00:00, 314.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Average Loss: 0.1604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [00:02<00:00, 334.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Average Loss: 0.1128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [00:01<00:00, 487.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Average Loss: 0.0862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 938/938 [00:02<00:00, 447.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Average Loss: 0.0690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# We train the model for several epochs (iterations over the entire training set)\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    # Wrap the DataLoader with tqdm to display a progress bar\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        # Move data to device (CPU or GPU)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass: pass images through the network\n",
    "        outputs = model(images)\n",
    "        # Compute the loss (how far predictions are from actual labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass: compute gradients of loss w.r.t model parameters\n",
    "        optimizer.zero_grad() # Clear previous gradients to prevent accumulation\n",
    "        loss.backward() # Compute new gradients\n",
    "        optimizer.step() # Update model parameters using gradients\n",
    "        \n",
    "        # Add the batch loss to the running total\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Compute and print average loss for the epoch\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.32%\n"
     ]
    }
   ],
   "source": [
    "# Before evaluating, we set the model to evaluation mode. This disables certain layers like dropout\n",
    "# and batch normalization which behave differently during training.\n",
    "model.eval()\n",
    "\n",
    "# Initialize counters to track correct predictions and total number of samples\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Wrap evaluation in torch.no_grad() context to avoid computing gradients, which saves memory and computation\n",
    "with torch.no_grad():\n",
    "    # Loop over test dataset in batches\n",
    "    for images, labels in test_loader:\n",
    "        # Move test data to the same device as the model\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass: compute the model outputs (predictions)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Get the predicted class for each image by finding the index with the maximum score\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Update the total number of samples seen so far\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # Count how many predictions matched the true labels\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Compute overall accuracy on the test set and print it\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
